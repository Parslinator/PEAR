{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Ratings Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "formatted_date = datetime.today().strftime('%m_%d_%Y')\n",
    "current_season = datetime.today().year\n",
    "\n",
    "def PEAR_Win_Prob(home_pr, away_pr):\n",
    "    rating_diff = home_pr - away_pr\n",
    "    win_prob = round(1 / (1 + 10 ** (-rating_diff / 7.5)) * 100, 2)\n",
    "    return win_prob\n",
    "\n",
    "# Base URL for NCAA stats\n",
    "base_url = \"https://www.ncaa.com\"\n",
    "stats_page = f\"{base_url}/stats/baseball/d1\"\n",
    "\n",
    "# Function to get page content\n",
    "def get_soup(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()  # Ensure request was successful\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Get main page content\n",
    "soup = get_soup(stats_page)\n",
    "\n",
    "# Find the dropdown container and extract stat URLs\n",
    "dropdown = soup.find(\"select\", {\"id\": \"select-container-team\"})\n",
    "options = dropdown.find_all(\"option\")\n",
    "\n",
    "# Extract stat names and links\n",
    "stat_links = {\n",
    "    option.text.strip(): base_url + option[\"value\"]\n",
    "    for option in options if option.get(\"value\")\n",
    "}\n",
    "\n",
    "url = \"https://www.ncaa.com/rankings/baseball/d1/rpi\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure request was successful\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"sticky\")\n",
    "if table:\n",
    "    headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "    data = []\n",
    "    for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        data.append([col.text.strip() for col in cols])\n",
    "    rpi = pd.DataFrame(data, columns=headers)\n",
    "    rpi = rpi.drop(columns = ['Previous'])\n",
    "    rpi.rename(columns={\"School\": \"Team\"}, inplace=True)\n",
    "else:\n",
    "    print(\"Table not found.\")\n",
    "\n",
    "url = \"https://www.collegebaseballratings.com/\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an error for failed requests\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"id\": \"teamList\"})\n",
    "headers = [th.text.strip() for th in table.find(\"thead\").find_all(\"th\")]\n",
    "data = []\n",
    "for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "    cells = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "    data.append(cells)\n",
    "cbr = pd.DataFrame(data, columns=headers[1:])\n",
    "cbr.rename(columns={\"Rank\":\"CBRank\"}, inplace=True)\n",
    "cbr['Team'] = cbr['Team'].str.replace('State', 'St.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Southern Miss', 'Southern Miss.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('NC St.', 'NC State', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Appalachian St.', 'App State', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Dallas Baptist', 'DBU', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('USC', 'Southern California', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Charleston', 'Col. of Charleston', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Col. of Charleston Southern', 'Charleston So.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Georgia Southern', 'Ga. Southern', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('UNC Wilmington', 'UNCW', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Southern Illinois', 'Southern Ill.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Florida Atlantic', 'Fla. Atlantic', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Lamar', 'Lamar University', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Western Kentucky', 'Western Ky.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Southern California Upstate', 'USC Upstate', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Southeast Missouri', 'Southeast Mo. St.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace(\"St. John's\", \"St. John's (NY)\", regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Southeastern Louisiana', 'Southeastern La.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Kennesaw', 'Kennesaw St.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Louisiana Monroe', 'ULM', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Western Carolina', 'Western Caro.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('USF', 'South Fla.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Loyola Marymount', 'LMU (CA)', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Army', 'Army West Point', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Incarnate Word', 'UIW', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Central Michigan', 'Central Mich.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Eastern Illinois', 'Eastern Ill.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Western Michigan', 'Western Mich.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Central Arkansas', 'Central Ark.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Middle Tennessee', 'Middle Tenn.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Monmouth (NJ)', 'Monmouth', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Northern Kentucky', 'Northern Ky.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('North Carolina A&T', 'N.C. A&T', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Texas A&M-Corpus Christi', 'A&M-Corpus Christi', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace(\"Saint Joseph's (PA)\", \"Saint Joseph's\", regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Eastern Kentucky', 'Eastern Ky.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Seattle', 'Seattle U', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Eastern Michigan', 'Eastern Mich.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('North Alabama', 'North Ala.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Northern Colorado', 'Northern Colo.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Stephen F. Austin', 'SFA', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Western Illinois', 'Western Ill.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Prairie View A&M', 'Prairie View', regex=False)\n",
    "cbr['Team'] = cbr['Team'].apply(lambda x: 'Southern U.' if x == 'Southern' else x)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Arkansas-Pine Bluff', 'Ark.-Pine Bluff', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Maryland Eastern Shore', 'UMES', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Mississippi Valley St.', 'Mississippi Val.', regex=False)\n",
    "cbr['Team'] = cbr['Team'].str.replace('Alcorn St.', 'Alcorn', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batting Average Done\n",
      "Base on Balls Done\n",
      "Double Plays Per Game Done\n",
      "Earned Run Average Done\n",
      "Fielding Percentage Done\n",
      "Hits Allowed Per Nine Innings Done\n",
      "Home Runs Per Game Done\n",
      "On Base Percentage Done\n",
      "Runs Done\n",
      "Sacrifice Bunts Done\n",
      "Sacrifice Flies Done\n",
      "Slugging Percentage Done\n",
      "Stolen Bases Done\n",
      "Strikeout-to-Walk Ratio Done\n",
      "Strikeouts Per Nine Innings Done\n",
      "Walks Allowed Per Nine Innings Done\n",
      "WHIP Done\n"
     ]
    }
   ],
   "source": [
    "def get_stat_dataframe(stat_name):\n",
    "    \"\"\"Fetches the specified stat table from multiple pages and returns a combined DataFrame,\n",
    "    keeps 'Team' as string, and converts all other columns to float.\"\"\"\n",
    "    \n",
    "    if stat_name not in stat_links:\n",
    "        print(f\"Stat '{stat_name}' not found. Available stats: {list(stat_links.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize the DataFrame to store all pages' data\n",
    "    all_data = []\n",
    "    page_num = 1  # Start from the first page\n",
    "\n",
    "    while True:\n",
    "        url = stat_links[stat_name]\n",
    "        if page_num > 1:\n",
    "            # Modify the URL to include the page number\n",
    "            url = f\"{url}/p{page_num}\"\n",
    "        \n",
    "        # print(f\"Fetching data for: {stat_name} (Page {page_num} - {url})\")\n",
    "\n",
    "        try:\n",
    "            # Get stats page content\n",
    "            soup = get_soup(url)\n",
    "\n",
    "            # Locate table\n",
    "            table = soup.find(\"table\")\n",
    "            if not table:\n",
    "                print(f\"No table found for {stat_name} on page {page_num}\")\n",
    "                break  # Exit the loop if no table is found (end of valid pages)\n",
    "\n",
    "            # Extract table headers\n",
    "            headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "\n",
    "            # Extract table rows\n",
    "            data = []\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "                cols = row.find_all(\"td\")\n",
    "                data.append([col.text.strip() for col in cols])\n",
    "\n",
    "            all_data.extend(data)  # Add the data from this page to the list of all data\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"{stat_name} Done\")\n",
    "            break  # Exit the loop on HTTPError (page doesn't exist)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break  # Exit the loop on any other error\n",
    "\n",
    "        page_num += 1  # Go to the next page\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data, columns=headers)\n",
    "\n",
    "        # Convert all columns to float except \"Team\"\n",
    "        for col in df.columns:\n",
    "            if col != \"Team\":\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # Converts to float, invalid values become NaN\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "stat_name_input = \"Batting Average\"  # Change this to the desired stat\n",
    "ba = get_stat_dataframe(stat_name_input)\n",
    "ba[\"HPG\"] = ba[\"H\"] / ba[\"G\"]\n",
    "ba[\"ABPG\"] = ba[\"AB\"] / ba[\"G\"]\n",
    "ba[\"HPAB\"] = ba[\"H\"] / ba[\"AB\"]\n",
    "ba = ba.drop(columns=['Rank'])\n",
    "\n",
    "stat_name_input = \"Base on Balls\"\n",
    "bb = get_stat_dataframe(stat_name_input)\n",
    "bb[\"BBPG\"] = bb[\"BB\"] / bb[\"G\"]\n",
    "bb = bb.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Double Plays Per Game\"\n",
    "dp = get_stat_dataframe(stat_name_input)\n",
    "dp.rename(columns={\"PG\": \"DPPG\"}, inplace=True)\n",
    "dp = dp.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Earned Run Average\"\n",
    "era = get_stat_dataframe(stat_name_input)\n",
    "era.rename(columns={\"R\":\"RA\"}, inplace=True)\n",
    "era = era.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Fielding Percentage\"\n",
    "fp = get_stat_dataframe(stat_name_input)\n",
    "fp[\"APG\"] = fp[\"A\"] / fp[\"G\"]\n",
    "fp[\"EPG\"] = fp[\"E\"] / fp[\"G\"]\n",
    "fp = fp.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Hits Allowed Per Nine Innings\"\n",
    "ha = get_stat_dataframe(stat_name_input)\n",
    "ha.rename(columns={\"PG\": \"HAPG\"}, inplace=True)\n",
    "ha = ha.drop(columns=['Rank', 'G', 'IP'])\n",
    "\n",
    "stat_name_input = \"Home Runs Per Game\"\n",
    "hr = get_stat_dataframe(stat_name_input)\n",
    "hr.rename(columns={\"PG\": \"HRPG\"}, inplace=True)\n",
    "hr = hr.drop(columns=['Rank', 'G'])\n",
    "duplicate_teams = hr[hr.duplicated('Team', keep=False)]\n",
    "filtered_teams = duplicate_teams.loc[duplicate_teams.groupby('Team')[\"HR\"].idxmin()]\n",
    "hr_cleaned = hr[~hr[\"Team\"].isin(duplicate_teams[\"Team\"])]\n",
    "hr = pd.concat([hr_cleaned, filtered_teams], ignore_index=True)\n",
    "\n",
    "stat_name_input = \"On Base Percentage\"\n",
    "obp = get_stat_dataframe(stat_name_input)\n",
    "obp.rename(columns={\"PCT\": \"OBP\"}, inplace=True)\n",
    "obp[\"HBPPG\"] = obp[\"HBP\"] / obp[\"G\"]\n",
    "obp = obp.drop(columns=['Rank', 'G', 'AB', 'H', 'BB', 'SF', 'SH'])\n",
    "\n",
    "stat_name_input = \"Runs\"\n",
    "runs = get_stat_dataframe(stat_name_input)\n",
    "runs[\"RPG\"] = runs[\"R\"] / runs[\"G\"]\n",
    "runs.rename(columns={\"R\": \"RS\"}, inplace=True)\n",
    "runs = runs.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Sacrifice Bunts\"\n",
    "sb = get_stat_dataframe(stat_name_input)\n",
    "sb.rename(columns={\"SH\": \"SB\"}, inplace=True)\n",
    "sb[\"SBPG\"] = sb[\"SB\"] / sb[\"G\"]\n",
    "sb = sb.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Sacrifice Flies\"\n",
    "sf = get_stat_dataframe(stat_name_input)\n",
    "sf[\"SFPG\"] = sf[\"SF\"] / sf[\"G\"]\n",
    "sf = sf.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Slugging Percentage\"\n",
    "slg = get_stat_dataframe(stat_name_input)\n",
    "slg.rename(columns={\"SLG PCT\": \"SLG\"}, inplace=True)\n",
    "slg = slg.drop(columns=['Rank', 'G', 'AB'])\n",
    "\n",
    "stat_name_input = \"Stolen Bases\"\n",
    "stl = get_stat_dataframe(stat_name_input)\n",
    "stl[\"STLP\"] = stl[\"SB\"] / (stl[\"SB\"] + stl[\"CS\"])\n",
    "stl[\"STLPG\"] = stl[\"SB\"] / stl[\"G\"]\n",
    "stl[\"CSPG\"] = stl[\"CS\"] / stl[\"G\"]\n",
    "stl[\"SAPG\"] = (stl[\"SB\"] + stl[\"CS\"]) / stl[\"G\"]\n",
    "stl.rename(columns={\"SB\": \"STL\"}, inplace=True)\n",
    "stl = stl.drop(columns=['Rank', 'G'])\n",
    "\n",
    "stat_name_input = \"Strikeout-to-Walk Ratio\"\n",
    "kbb = get_stat_dataframe(stat_name_input)\n",
    "kbb[\"IP\"] = round(kbb[\"IP\"])\n",
    "kbb.rename(columns={\"K/BB\": \"KBB\"}, inplace=True)\n",
    "kbb.rename(columns={\"BB\": \"PBB\"}, inplace=True)\n",
    "kbb = kbb.drop(columns=['Rank', 'App', 'IP'])\n",
    "\n",
    "stat_name_input = \"Strikeouts Per Nine Innings\"\n",
    "kp9 = get_stat_dataframe(stat_name_input)\n",
    "kp9.rename(columns={\"K/9\": \"KP9\"}, inplace=True)\n",
    "kp9 = kp9.drop(columns=['Rank', 'G', 'IP', 'SO'])\n",
    "\n",
    "stat_name_input = \"Walks Allowed Per Nine Innings\"\n",
    "wp9 = get_stat_dataframe(stat_name_input)\n",
    "wp9.rename(columns={\"PG\": \"WP9\"}, inplace=True)\n",
    "wp9 = wp9.drop(columns=['Rank', 'G', 'IP', 'BB'])\n",
    "\n",
    "stat_name_input = \"WHIP\"\n",
    "whip = get_stat_dataframe(stat_name_input)\n",
    "whip = whip.drop(columns=['Rank', 'HA', 'IP', 'BB'])\n",
    "\n",
    "dfs = [ba, bb, era, fp, obp, runs, slg, kp9, wp9, whip, cbr]\n",
    "for df in dfs:\n",
    "    df[\"Team\"] = df[\"Team\"].str.strip()\n",
    "df_combined = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    df_combined = pd.merge(df_combined, df, on=\"Team\", how=\"inner\")\n",
    "baseball_stats = df_combined.loc[:, ~df_combined.columns.duplicated()].sort_values('Team').reset_index(drop=True)\n",
    "baseball_stats['OPS'] = baseball_stats['SLG'] + baseball_stats['OBP']\n",
    "baseball_stats['PYTHAG'] = (baseball_stats['RS'] ** 1.83) / ((baseball_stats['RS'] ** 1.83) + (baseball_stats['RA'] ** 1.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEAR Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpi_2024 = pd.read_csv(\"./PEAR/PEAR Baseball/rpi_end_2024.csv\")\n",
    "\n",
    "modeling_stats = baseball_stats[['Team', 'HPG',\n",
    "                'BBPG', 'ERA', 'PCT', \n",
    "                'KP9', 'WP9', 'OPS', \n",
    "                'WHIP', 'PYTHAG', 'CBRank']]\n",
    "modeling_stats = pd.merge(modeling_stats, rpi_2024[['Team', 'Rank']], on = 'Team', how='left')\n",
    "modeling_stats[\"Rank\"] = modeling_stats[\"Rank\"].apply(pd.to_numeric, errors='coerce')\n",
    "modeling_stats[\"CBRank\"] = modeling_stats[\"CBRank\"].apply(pd.to_numeric, errors='coerce')\n",
    "modeling_stats['Rank_pct'] = 1 - (modeling_stats['Rank'] - 1) / (len(modeling_stats) - 1)\n",
    "\n",
    "higher_better = [\"HPG\", \"BBPG\", \"PCT\", \"KP9\", \"OPS\", \"Rank_pct\", 'PYTHAG']\n",
    "lower_better = [\"ERA\", \"WP9\", \"WHIP\"]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "modeling_stats[higher_better] = scaler.fit_transform(modeling_stats[higher_better])\n",
    "modeling_stats[lower_better] = scaler.fit_transform(-modeling_stats[lower_better])\n",
    "weights = {\n",
    "    'HPG': 8, 'BBPG': 8, 'ERA': 22, 'PCT': 8,\n",
    "    'KP9': 8, 'WP9': 8, 'OPS': 22, 'WHIP': 8, 'PYTHAG': 22, 'Rank_pct': 50\n",
    "}\n",
    "modeling_stats['in_house_pr'] = sum(modeling_stats[stat] * weight for stat, weight in weights.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_stats['in_house_pr'] = modeling_stats['in_house_pr'] - modeling_stats['in_house_pr'].mean()\n",
    "current_range = modeling_stats['in_house_pr'].max() - modeling_stats['in_house_pr'].min()\n",
    "desired_range = 25\n",
    "scaling_factor = desired_range / current_range\n",
    "modeling_stats['in_house_pr'] = round(modeling_stats['in_house_pr'] * scaling_factor, 4)\n",
    "modeling_stats['in_house_pr'] = modeling_stats['in_house_pr'] - modeling_stats['in_house_pr'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  79%|███████▉  | 395/500 [05:22<00:46,  2.26it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=500, desc=\"Optimization Progress\")\n",
    "def progress_callback(xk, convergence):\n",
    "    \"\"\"Callback to update the progress bar after each iteration.\"\"\"\n",
    "    pbar.update(1)\n",
    "    if convergence < 1e-4:  # Close bar if convergence is achieved early\n",
    "        pbar.close()\n",
    "\n",
    "def objective_function(weights):\n",
    "    (w_hpb, w_bbpg, w_era, w_pct, w_kp9, w_wp9, w_whip, w_ops, w_pythag, w_in_house_pr) = weights\n",
    "    \n",
    "    modeling_stats['power_ranking'] = (\n",
    "        w_hpb * modeling_stats['HPG'] +\n",
    "        w_bbpg * modeling_stats['BBPG'] +\n",
    "        w_era * modeling_stats['ERA'] +\n",
    "        w_pct * modeling_stats['PCT'] +\n",
    "        w_kp9 * modeling_stats['KP9'] +\n",
    "        w_wp9 * modeling_stats['WP9'] +\n",
    "        w_whip * modeling_stats['WHIP'] +\n",
    "        w_ops * modeling_stats['OPS'] +\n",
    "        w_pythag * modeling_stats['PYTHAG'] + \n",
    "        w_in_house_pr * modeling_stats['in_house_pr']\n",
    "    )\n",
    "\n",
    "    modeling_stats['calculated_rank'] = modeling_stats['power_ranking'].rank(ascending=False)\n",
    "    modeling_stats['combined_rank'] = (\n",
    "        modeling_stats['CBRank']\n",
    "    )\n",
    "    spearman_corr = modeling_stats[['calculated_rank', 'combined_rank']].corr(method='spearman').iloc[0,1]\n",
    "\n",
    "    return -spearman_corr\n",
    "\n",
    "bounds = [(-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (-1,1),\n",
    "          (0,1)]\n",
    "result = differential_evolution(objective_function, bounds, strategy='best1bin', maxiter=500, tol=1e-4, seed=42, callback=progress_callback)\n",
    "optimized_weights = result.x\n",
    "modeling_stats = modeling_stats.sort_values('power_ranking', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_stats['Rating'] = modeling_stats['power_ranking'] - modeling_stats['power_ranking'].mean()\n",
    "current_range = modeling_stats['Rating'].max() - modeling_stats['Rating'].min()\n",
    "desired_range = 15\n",
    "scaling_factor = desired_range / current_range\n",
    "modeling_stats['Rating'] = round(modeling_stats['Rating'] * scaling_factor, 4)\n",
    "modeling_stats['Rating'] = modeling_stats['Rating'] - modeling_stats['Rating'].min()\n",
    "modeling_stats['Rating'] = round(modeling_stats['Rating'] - modeling_stats['Rating'].mean(),2)\n",
    "modeling_stats['Rating'] = round(modeling_stats['Rating'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_data = pd.merge(baseball_stats, modeling_stats[['Team', 'Rating']], on=\"Team\", how=\"inner\").sort_values('Rating', ascending=False).reset_index(drop=True)\n",
    "ending_data = ending_data.drop(columns=['SOR', 'SOS'])\n",
    "ending_data.index = ending_data.index + 1\n",
    "ending_data[['Wins', 'Losses']] = ending_data['Rec'].str.split('-', expand=True).astype(int)\n",
    "ending_data['WIN%'] = round(ending_data['Wins'] / (ending_data['Wins'] + ending_data['Losses']), 3)\n",
    "ending_data['Wins_Over_Pythag'] = ending_data['WIN%'] - ending_data['PYTHAG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule Info Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.warrennolan.com/baseball/2025/elo'\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table with the specified class\n",
    "table = soup.find('table', class_='normal-grid alternating-rows stats-table')\n",
    "\n",
    "if table:\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "    headers.insert(1, \"Team Link\")  # Adding extra column for team link\n",
    "\n",
    "    # Extract table rows\n",
    "    data = []\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        row_data = []\n",
    "        for i, cell in enumerate(cells):\n",
    "            # If it's the first cell, extract team name and link from 'name-subcontainer'\n",
    "            if i == 0:\n",
    "                name_container = cell.find('div', class_='name-subcontainer')\n",
    "                if name_container:\n",
    "                    team_name = name_container.text.strip()\n",
    "                    team_link_tag = name_container.find('a')\n",
    "                    team_link = team_link_tag['href'] if team_link_tag else ''\n",
    "                else:\n",
    "                    team_name = cell.text.strip()\n",
    "                    team_link = ''\n",
    "                row_data.append(team_name)\n",
    "                row_data.append(team_link)  # Add team link separately\n",
    "            else:\n",
    "                row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "\n",
    "\n",
    "    elo_data = pd.DataFrame(data, columns=[headers])\n",
    "    elo_data.columns = elo_data.columns.get_level_values(0)\n",
    "    elo_data = elo_data.drop_duplicates(subset='Team', keep='first')\n",
    "    elo_data = elo_data.astype({col: 'str' for col in elo_data.columns if col not in ['ELO', 'Rank']})\n",
    "    elo_data['ELO'] = elo_data['ELO'].astype(float, errors='ignore')\n",
    "    elo_data['Rank'] = elo_data['Rank'].astype(int, errors='ignore')\n",
    "\n",
    "else:\n",
    "    print(\"Table not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texas A&M\n",
      "Tennessee\n",
      "North Carolina\n",
      "LSU\n",
      "Oregon State\n",
      "Arkansas\n",
      "Florida State\n",
      "Virginia\n",
      "Florida\n",
      "Georgia\n",
      "Vanderbilt\n",
      "Oregon\n",
      "Wake Forest\n",
      "Duke\n",
      "Clemson\n",
      "TCU\n",
      "North Carolina State\n",
      "Texas\n",
      "Mississippi State\n",
      "Southern Miss\n",
      "South Carolina\n",
      "USC\n",
      "UC Santa Barbara\n",
      "Hawaii\n",
      "Georgia Tech\n",
      "Dallas Baptist\n",
      "Arizona State\n",
      "Michigan\n",
      "Oklahoma\n",
      "Charleston\n",
      "Oklahoma State\n",
      "UCF\n",
      "Alabama\n",
      "West Virginia\n",
      "Louisiana Tech\n",
      "Coastal Carolina\n",
      "Troy\n",
      "Kansas\n",
      "California\n",
      "Nebraska\n",
      "Louisville\n",
      "High Point\n",
      "Tulane\n",
      "Cincinnati\n",
      "Georgia Southern\n",
      "Auburn\n",
      "Indiana State\n",
      "Stetson\n",
      "Arizona\n",
      "UNCG\n",
      "UC Irvine\n",
      "Kentucky\n",
      "Wofford\n",
      "Ole Miss\n",
      "Illinois\n",
      "Miami (FL)\n",
      "Liberty\n",
      "Pittsburgh\n",
      "South Alabama\n",
      "UNCW\n",
      "East Tennessee State\n",
      "Lamar\n",
      "Texas State\n",
      "Utah\n",
      "Saint Mary's College\n",
      "Michigan State\n",
      "East Carolina\n",
      "Louisiana\n",
      "Western Kentucky\n",
      "Virginia Tech\n",
      "Murray State\n",
      "California Baptist\n",
      "Northeastern\n",
      "Penn State\n",
      "FAU\n",
      "Grand Canyon\n",
      "Samford\n",
      "James Madison\n",
      "Baylor\n",
      "Purdue\n",
      "UCLA\n",
      "Xavier\n",
      "Georgetown\n",
      "San Diego\n",
      "Portland\n",
      "Stanford\n",
      "Houston\n",
      "McNeese\n",
      "Notre Dame\n",
      "Maryland\n",
      "Kansas State\n",
      "Creighton\n",
      "San Jose State\n",
      "FIU\n",
      "Connecticut\n",
      "Little Rock\n",
      "UTRGV\n",
      "Texas Tech\n",
      "Ball State\n",
      "Nicholls\n",
      "Wichita State\n",
      "Fairfield\n",
      "Southeast Missouri\n",
      "Campbell\n",
      "Evansville\n",
      "Appalachian State\n",
      "Cal Poly\n",
      "Iowa\n",
      "Morehead State\n",
      "VCU\n",
      "UC Davis\n",
      "Old Dominion\n",
      "Missouri\n",
      "Saint John's\n",
      "Washington\n",
      "Rutgers\n",
      "Bowling Green\n",
      "Bryant\n",
      "South Carolina Upstate\n",
      "Tarleton State\n",
      "Indiana\n",
      "Columbia\n",
      "Richmond\n",
      "Minnesota\n",
      "Santa Clara\n",
      "Southeastern Louisiana\n",
      "Georgia State\n",
      "Sam Houston State\n",
      "BYU\n",
      "UAB\n",
      "ULM\n",
      "Charlotte\n",
      "Mercer\n",
      "Boston College\n",
      "Western Carolina\n",
      "Ohio State\n",
      "Cal State Northridge\n",
      "Wright State\n",
      "FGCU\n",
      "Long Island\n",
      "Austin Peay\n",
      "Loyola-Marymount\n",
      "Niagara\n",
      "New Mexico State\n",
      "UIC\n",
      "Kennesaw State\n",
      "UTSA\n",
      "Saint Louis\n",
      "Saint Joseph's\n",
      "South Florida\n",
      "Memphis\n",
      "VMI\n",
      "Delaware\n",
      "Presbyterian College\n",
      "Oral Roberts\n",
      "Nevada\n",
      "Stony Brook\n",
      "Abilene Christian\n",
      "Rice\n",
      "Illinois State\n",
      "New Mexico\n",
      "Northwestern State\n",
      "Army\n",
      "UNLV\n",
      "Jacksonville State\n",
      "Lipscomb\n",
      "Dayton\n",
      "UC San Diego\n",
      "New Orleans\n",
      "Missouri State\n",
      "Hofstra\n",
      "Elon\n",
      "Jackson State\n",
      "Winthrop\n",
      "Southern Illinois\n",
      "UNC Asheville\n",
      "Rider\n",
      "Tennessee Tech\n",
      "Charleston Southern\n",
      "UMBC\n",
      "Rhode Island\n",
      "William & Mary\n",
      "Air Force\n",
      "North Carolina A&T\n",
      "Central Arkansas\n",
      "Southern Indiana\n",
      "Fresno State\n",
      "Miami (OH)\n",
      "San Diego State\n",
      "Penn\n",
      "Utah Valley\n",
      "Kent State\n",
      "Western Michigan\n",
      "Northwestern\n",
      "Northern Kentucky\n",
      "UMass\n",
      "UTA\n",
      "Bucknell\n",
      "Monmouth\n",
      "Sacred Heart\n",
      "Gonzaga\n",
      "Incarnate Word\n",
      "Houston Christian\n",
      "Grambling State\n",
      "George Washington\n",
      "Gardner-Webb\n",
      "Seton Hall\n",
      "Long Beach State\n",
      "Holy Cross\n",
      "Mount Saint Mary's\n",
      "Tennessee-Martin\n",
      "Merrimack\n",
      "West Georgia\n",
      "The Citadel\n",
      "Toledo\n",
      "Saint Thomas\n",
      "UMass-Lowell\n",
      "Jacksonville\n",
      "Northern Illinois\n",
      "Villanova\n",
      "Saint Bonaventure\n",
      "Yale\n",
      "Sacramento State\n",
      "Longwood\n",
      "Princeton\n",
      "Washington State\n",
      "SIUE\n",
      "Cornell\n",
      "North Florida\n",
      "Davidson\n",
      "Middle Tennessee\n",
      "Oakland\n",
      "North Alabama\n",
      "NJIT\n",
      "Alabama State\n",
      "Manhattan\n",
      "Navy\n",
      "UC Riverside\n",
      "Bethune-Cookman\n",
      "Wagner\n",
      "Arkansas State\n",
      "Eastern Illinois\n",
      "Butler\n",
      "Cal State Bakersfield\n",
      "Quinnipiac\n",
      "Central Connecticut\n",
      "Towson\n",
      "Prairie View A&M\n",
      "Marshall\n",
      "San Francisco\n",
      "Queens\n",
      "Akron\n",
      "George Mason\n",
      "Pacific\n",
      "Harvard\n",
      "Texas Southern\n",
      "Northern Colorado\n",
      "Texas A&M-Corpus Christi\n",
      "Ohio\n",
      "Belmont\n",
      "Eastern Kentucky\n",
      "Southern\n",
      "Florida A&M\n",
      "Omaha\n",
      "Lehigh\n",
      "Utah Tech\n",
      "Milwaukee\n",
      "Mercyhurst\n",
      "Fairleigh Dickinson\n",
      "Albany\n",
      "Cal State Fullerton\n",
      "South Dakota State\n",
      "Delaware State\n",
      "Dartmouth\n",
      "Canisius\n",
      "Binghamton\n",
      "Brown\n",
      "Purdue Fort Wayne\n",
      "Fordham\n",
      "North Dakota State\n",
      "Radford\n",
      "Youngstown State\n",
      "Seattle University\n",
      "Iona\n",
      "Pepperdine\n",
      "Mississippi Valley State\n",
      "Valparaiso\n",
      "Central Michigan\n",
      "Coppin State\n",
      "Bradley\n",
      "Lafayette\n",
      "Western Illinois\n",
      "Marist\n",
      "Alabama A&M\n",
      "Bellarmine\n",
      "Siena\n",
      "Saint Peter's\n",
      "Maine\n",
      "Eastern Michigan\n",
      "Alcorn State\n",
      "Stonehill\n",
      "Le Moyne\n",
      "Stephen F. Austin\n",
      "Lindenwood\n",
      "Arkansas-Pine Bluff\n",
      "Norfolk State\n",
      "Maryland Eastern Shore\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL for Warren Nolan\n",
    "BASE_URL = \"https://www.warrennolan.com\"\n",
    "\n",
    "# Initialize storage for schedule data\n",
    "schedule_data = []\n",
    "\n",
    "# Iterate over each team's schedule link\n",
    "for _, row in elo_data.iterrows():\n",
    "    team_name = row[\"Team\"]\n",
    "    print(team_name)\n",
    "    team_schedule_url = BASE_URL + row[\"Team Link\"]\n",
    "    \n",
    "    response = requests.get(team_schedule_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the team name\n",
    "    # team_name = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"Unknown\"\n",
    "\n",
    "    # Find the team schedule list\n",
    "    schedule_lists = soup.find_all(\"ul\", class_=\"team-schedule\")\n",
    "    if not schedule_lists:\n",
    "        continue  # Skip if no schedule is found\n",
    "\n",
    "    schedule_list = schedule_lists[0]\n",
    "\n",
    "    # Iterate over each game row in the schedule\n",
    "    for game in schedule_list.find_all('li', class_='team-schedule'):\n",
    "        # Extract Date\n",
    "        date_month = game.find('span', class_='team-schedule__game-date--month').text.strip()\n",
    "        date_day = game.find('span', class_='team-schedule__game-date--day').text.strip()\n",
    "        date_dow = game.find('span', class_='team-schedule__game-date--dow').text.strip()\n",
    "        game_date = f\"{date_month} {date_day} ({date_dow})\"\n",
    "\n",
    "        # Extract Opponent Name (Handle missing cases)\n",
    "        opponent_info = game.find('div', class_='team-schedule__opp')\n",
    "        if opponent_info:\n",
    "            opponent_link_element = opponent_info.find('a', class_='team-schedule__opp-line-link')\n",
    "            opponent_name = opponent_link_element.text.strip() if opponent_link_element else \"\"\n",
    "        else:\n",
    "            opponent_name = \"\"\n",
    "\n",
    "        # Extract Location\n",
    "        location_info = game.find('div', class_='team-schedule__info')\n",
    "        location = location_info.text.strip() if location_info else \"Unknown\"\n",
    "\n",
    "        # Extract Game Result\n",
    "        result_info = game.find('div', class_='team-schedule__result')\n",
    "        result_text = result_info.text.strip() if result_info else \"N/A\"\n",
    "\n",
    "        # Extract Home/Away Teams from Box Score and scores\n",
    "        home_score, away_score = \"\", \"\"  # Initialize scores as empty strings\n",
    "\n",
    "        box_score_table = game.find('table', class_='team-schedule-bottom__box-score')\n",
    "        if box_score_table:\n",
    "            rows = box_score_table.find_all('tr')\n",
    "            if len(rows) > 2:\n",
    "                away_team = rows[1].find_all('td')[0].text.strip()\n",
    "                home_team = rows[2].find_all('td')[0].text.strip()\n",
    "\n",
    "                # Extracting Runs\n",
    "                away_score = rows[1].find_all('td')[-3].text.strip()  # Away runs\n",
    "                home_score = rows[2].find_all('td')[-3].text.strip()  # Home runs\n",
    "            else:\n",
    "                home_team, away_team = \"N/A\", \"N/A\"\n",
    "        else:\n",
    "            home_team, away_team = \"N/A\", \"N/A\"\n",
    "\n",
    "        # Append to schedule data\n",
    "        schedule_data.append([team_name, game_date, opponent_name, location, result_text, home_team, away_team, home_score, away_score])\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [\"Team\", \"Date\", \"Opponent\", \"Location\", \"Result\", \"home_team\", \"away_team\", \"home_score\", \"away_score\"]\n",
    "schedule_df = pd.DataFrame(schedule_data, columns=columns)\n",
    "schedule_df = schedule_df.astype({col: 'str' for col in schedule_df.columns if col not in ['home_score', 'away_score']})\n",
    "schedule_df['home_score'] = schedule_df['home_score'].astype(int, errors='ignore')\n",
    "schedule_df['away_score'] = schedule_df['away_score'].astype(int, errors='ignore')\n",
    "schedule_df = schedule_df.merge(elo_data[['Team', 'ELO']], left_on='home_team', right_on='Team', how='left')\n",
    "schedule_df.rename(columns={'ELO': 'home_elo'}, inplace=True)\n",
    "schedule_df = schedule_df.merge(elo_data[['Team', 'ELO']], left_on='away_team', right_on='Team', how='left')\n",
    "schedule_df.rename(columns={'ELO': 'away_elo'}, inplace=True)\n",
    "schedule_df.drop(columns=['Team', 'Team_y'], inplace=True)\n",
    "schedule_df.rename(columns={'Team_x':'Team'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.warrennolan.com/baseball/2025/elo'\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table with the specified class\n",
    "table = soup.find('table', class_='normal-grid alternating-rows stats-table')\n",
    "\n",
    "if table:\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "    headers.insert(1, \"Team Link\")  # Adding extra column for team link\n",
    "\n",
    "    # Extract table rows\n",
    "    data = []\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        row_data = []\n",
    "        for i, cell in enumerate(cells):\n",
    "            # If it's the first cell, extract team name and link from 'name-subcontainer'\n",
    "            if i == 0:\n",
    "                name_container = cell.find('div', class_='name-subcontainer')\n",
    "                if name_container:\n",
    "                    team_name = name_container.text.strip()\n",
    "                    team_link_tag = name_container.find('a')\n",
    "                    team_link = team_link_tag['href'] if team_link_tag else ''\n",
    "                else:\n",
    "                    team_name = cell.text.strip()\n",
    "                    team_link = ''\n",
    "                row_data.append(team_name)\n",
    "                row_data.append(team_link)  # Add team link separately\n",
    "            else:\n",
    "                row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "\n",
    "\n",
    "    elo_data = pd.DataFrame(data, columns=[headers])\n",
    "    elo_data.columns = elo_data.columns.get_level_values(0)\n",
    "    elo_data = elo_data.drop_duplicates(subset='Team', keep='first')\n",
    "    elo_data = elo_data.astype({col: 'str' for col in elo_data.columns if col not in ['ELO', 'Rank']})\n",
    "    elo_data['ELO'] = elo_data['ELO'].astype(float, errors='ignore')\n",
    "    elo_data['Rank'] = elo_data['Rank'].astype(int, errors='ignore')\n",
    "\n",
    "else:\n",
    "    print(\"Table not found on the page.\")\n",
    "\n",
    "# Define mapping for team name replacements\n",
    "team_replacements = {\n",
    "    'North Carolina St.': 'NC State',\n",
    "    'Southern Miss': 'Southern Miss.',\n",
    "    'USC': 'Southern California',\n",
    "    'Dallas Baptist': 'DBU',\n",
    "    'Charleston': 'Col. of Charleston',\n",
    "    'Georgia Southern': 'Ga. Southern',\n",
    "    'UNCG': 'UNC Greensboro',\n",
    "    'East Tennessee St.': 'ETSU',\n",
    "    'Lamar': 'Lamar University',\n",
    "    \"Saint Mary's College\": \"Saint Mary's (CA)\",\n",
    "    'Western Kentucky': 'Western Ky.',\n",
    "    'FAU': 'Fla. Atlantic',\n",
    "    'Connecticut': 'UConn',\n",
    "    'Southeast Missouri': 'Southeast Mo. St.',\n",
    "    'Alcorn St.': 'Alcorn',\n",
    "    'Appalachian St.': 'App State',\n",
    "    'Arkansas-Pine Bluff': 'Ark.-Pine Bluff',\n",
    "    'Army': 'Army West Point',\n",
    "    'Cal St. Bakersfield': 'CSU Bakersfield',\n",
    "    'Cal St. Northridge': 'CSUN',\n",
    "    'Central Arkansas': 'Central Ark.',\n",
    "    'Central Michigan': 'Central Mich.',\n",
    "    'Charleston Southern': 'Charleston So.',\n",
    "    'Eastern Illinois': 'Eastern Ill.',\n",
    "    'Eastern Kentucky': 'Eastern Ky.',\n",
    "    'Eastern Michigan': 'Eastern Mich.',\n",
    "    'Fairleigh Dickinson': 'FDU',\n",
    "    'Grambling St.': 'Grambling',\n",
    "    'Incarnate Word': 'UIW',\n",
    "    'Long Island': 'LIU',\n",
    "    'Maryland Eastern Shore': 'UMES',\n",
    "    'Middle Tennessee': 'Middle Tenn.',\n",
    "    'Mississippi Valley St.': 'Mississippi Val.',\n",
    "    \"Mount Saint Mary's\": \"Mount St. Mary's\",\n",
    "    'North Alabama': 'North Ala.',\n",
    "    'North Carolina A&T': 'N.C. A&T',\n",
    "    'Northern Colorado': 'Northern Colo.',\n",
    "    'Northern Kentucky': 'Northern Ky.',\n",
    "    'Prairie View A&M': 'Prairie View',\n",
    "    'Presbyterian College': 'Presbyterian',\n",
    "    'Saint Bonaventure': 'St. Bonaventure',\n",
    "    \"Saint John's\": \"St. John's (NY)\",\n",
    "    'Sam Houston St.': 'Sam Houston',\n",
    "    'Seattle University': 'Seattle U',\n",
    "    'South Carolina Upstate': 'USC Upstate',\n",
    "    'South Florida': 'South Fla.',\n",
    "    'Southeastern Louisiana': 'Southeastern La.',\n",
    "    'Southern': 'Southern U.',\n",
    "    'Southern Illinois': 'Southern Ill.',\n",
    "    'Stephen F. Austin': 'SFA',\n",
    "    'Tennessee-Martin': 'UT Martin',\n",
    "    'Texas A&M-Corpus Christi': 'A&M-Corpus Christi',\n",
    "    'UMass-Lowell': 'UMass Lowell',\n",
    "    'UTA': 'UT Arlington',\n",
    "    'Western Carolina': 'Western Caro.',\n",
    "    'Western Illinois': 'Western Ill.',\n",
    "    'Western Michigan': 'Western Mich.',\n",
    "}\n",
    "\n",
    "# Apply replacements and standardize 'State' to 'St.'\n",
    "columns_to_replace = ['Team', 'home_team', 'away_team', 'Opponent']\n",
    "\n",
    "for col in columns_to_replace:\n",
    "    schedule_df[col] = schedule_df[col].str.replace('State', 'St.', regex=False)\n",
    "    schedule_df[col] = schedule_df[col].replace(team_replacements)\n",
    "elo_data['Team'] = elo_data['Team'].str.replace('State', 'St.', regex=False)\n",
    "elo_data['Team'] = elo_data['Team'].replace(team_replacements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping months to numerical values\n",
    "month_mapping = {\n",
    "    \"JAN\": \"01\", \"FEB\": \"02\", \"MAR\": \"03\", \"APR\": \"04\",\n",
    "    \"MAY\": \"05\", \"JUN\": \"06\", \"JUL\": \"07\", \"AUG\": \"08\",\n",
    "    \"SEP\": \"09\", \"OCT\": \"10\", \"NOV\": \"11\", \"DEC\": \"12\"\n",
    "}\n",
    "\n",
    "current_season = 2025  # Set the current season\n",
    "\n",
    "# Function to convert \"FEB 14 (FRI)\" format to \"mm-dd-yyyy\"\n",
    "def convert_date(date_str):\n",
    "    # Ensure date is a string before splitting\n",
    "    if isinstance(date_str, pd.Timestamp):\n",
    "        date_str = date_str.strftime(\"%b %d (%a)\").upper()  # Convert to same format\n",
    "    \n",
    "    parts = date_str.split()  # [\"FEB\", \"14\", \"(FRI)\"]\n",
    "    month = month_mapping[parts[0].upper()]  # Convert month to number\n",
    "    day = parts[1]  # Extract day\n",
    "    return f\"{month}-{day}-{current_season}\"\n",
    "\n",
    "# Apply function to convert date format\n",
    "schedule_df[\"Date\"] = schedule_df[\"Date\"].astype(str).apply(convert_date)\n",
    "schedule_df[\"Date\"] = pd.to_datetime(schedule_df[\"Date\"], format=\"%m-%d-%Y\")\n",
    "comparison_date = pd.to_datetime(formatted_date, format=\"%m_%d_%Y\")\n",
    "\n",
    "missing_rating = round(ending_data['Rating'].mean() - 1.5*ending_data['Rating'].std(),2)\n",
    "schedule_df = schedule_df.merge(ending_data[['Team', 'Rating']], left_on='home_team', right_on='Team', how='left')\n",
    "schedule_df.rename(columns={'Rating': 'home_rating'}, inplace=True)\n",
    "schedule_df = schedule_df.merge(ending_data[['Team', 'Rating']], left_on='away_team', right_on='Team', how='left')\n",
    "schedule_df.rename(columns={'Rating': 'away_rating'}, inplace=True)\n",
    "schedule_df.drop(columns=['Team', 'Team_y'], inplace=True)\n",
    "schedule_df.rename(columns={'Team_x':'Team'}, inplace=True)\n",
    "schedule_df['home_rating'].fillna(missing_rating, inplace=True)\n",
    "schedule_df['away_rating'].fillna(missing_rating, inplace=True)\n",
    "schedule_df['home_win_prob'] = schedule_df.apply(\n",
    "    lambda row: PEAR_Win_Prob(row['home_rating'], row['away_rating']) / 100, axis=1\n",
    ")\n",
    "completed_schedule = schedule_df[\n",
    "    (schedule_df[\"Date\"] < comparison_date) & (schedule_df[\"home_score\"] != schedule_df[\"away_score\"])\n",
    "].reset_index(drop=True)\n",
    "remaining_games = schedule_df[schedule_df[\"Date\"] > comparison_date].reset_index(drop=True)\n",
    "\n",
    "def adjust_home_pr(home_win_prob):\n",
    "    return ((home_win_prob - 50) / 50) * 1.5\n",
    "schedule_df['elo_win_prob'] = round((10**((schedule_df['home_elo'] - schedule_df['away_elo']) / 400)) / ((10**((schedule_df['home_elo'] - schedule_df['away_elo']) / 400)) + 1)*100,2)\n",
    "schedule_df['Spread'] = (schedule_df['home_rating'] + (schedule_df['elo_win_prob'].apply(adjust_home_pr)) - schedule_df['away_rating']).round(2)\n",
    "schedule_df['PEAR'] = schedule_df.apply(\n",
    "    lambda row: f\"{row['away_team']} {-abs(row['Spread'])}\" if ((row['Spread'] <= 0)) \n",
    "    else f\"{row['home_team']} {-abs(row['Spread'])}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_wins(group):\n",
    "    # Initialize a variable to accumulate expected wins\n",
    "    expected_wins = 0\n",
    "    schedule_wins = 0\n",
    "    schedule_losses = 0\n",
    "    \n",
    "    # Iterate over the rows of the group\n",
    "    for _, row in group.iterrows():\n",
    "        if row['Team'] == row['home_team']:\n",
    "            expected_wins += row['home_win_prob']\n",
    "            if row['home_score'] > row['away_score']:\n",
    "                schedule_wins += 1\n",
    "            else:\n",
    "                schedule_losses += 1\n",
    "        else:\n",
    "            expected_wins += 1 - row['home_win_prob']\n",
    "            if row['away_score'] > row['home_score']:\n",
    "                schedule_wins += 1\n",
    "            else:\n",
    "                schedule_losses += 1\n",
    "    \n",
    "    # Return the total expected_wins for this group\n",
    "    return pd.Series({'Team': group['Team'].iloc[0], 'expected_wins': expected_wins, 'Wins':schedule_wins, 'Losses':schedule_losses})\n",
    "\n",
    "# Group by 'Team' and apply the calculation\n",
    "team_expected_wins = completed_schedule.groupby('Team').apply(calculate_expected_wins).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_expected_wins(group, average_team):\n",
    "    avg_expected_wins = 0\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        if row['Team'] == row['home_team']:\n",
    "            avg_expected_wins += PEAR_Win_Prob(average_team, row['away_rating']) / 100\n",
    "        else:\n",
    "            avg_expected_wins += 1 - PEAR_Win_Prob(row['home_rating'], average_team) / 100\n",
    "\n",
    "    return pd.Series({'Team': group['Team'].iloc[0], 'avg_expected_wins': avg_expected_wins})\n",
    "\n",
    "average_team = ending_data['Rating'].mean()\n",
    "avg_team_expected_wins = completed_schedule.groupby('Team').apply(calculate_average_expected_wins, average_team).reset_index(drop=True)\n",
    "\n",
    "rem_avg_expected_wins = remaining_games.groupby('Team').apply(calculate_average_expected_wins, average_team).reset_index(drop=True)\n",
    "rem_avg_expected_wins.rename(columns={\"avg_expected_wins\": \"rem_avg_expected_wins\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadrant_records = {}\n",
    "\n",
    "for team, group in completed_schedule.groupby('Team'):\n",
    "    Q1_win, Q1_loss = 0, 0  # Initialize counters\n",
    "    Q2_win, Q2_loss = 0, 0\n",
    "    Q3_win, Q3_loss = 0, 0\n",
    "    Q4_win, Q4_loss = 0, 0\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        opponent = row['Opponent']\n",
    "        \n",
    "        if len(ending_data[ending_data['Team'] == opponent]) > 0:\n",
    "            opponent_index = ending_data[ending_data['Team'] == opponent].index.values[0]\n",
    "        else:\n",
    "            opponent_index = 300\n",
    "\n",
    "        team_is_home = row['Team'] == row['home_team']\n",
    "        team_won = (row['home_score'] > row['away_score'] and team_is_home) or \\\n",
    "                    (row['away_score'] > row['home_score'] and not team_is_home)\n",
    "\n",
    "        # Apply quadrant logic\n",
    "        if team_is_home and opponent_index <= 25:\n",
    "            if team_won:\n",
    "                Q1_win += 1\n",
    "            else:\n",
    "                Q1_loss += 1\n",
    "        elif team_is_home and opponent_index <= 50:\n",
    "            if team_won:\n",
    "                Q2_win += 1\n",
    "            else:\n",
    "                Q2_loss += 1\n",
    "        elif team_is_home and opponent_index <= 100:\n",
    "            if team_won:\n",
    "                Q3_win += 1\n",
    "            else:\n",
    "                Q3_loss += 1\n",
    "        elif team_is_home:\n",
    "            if team_won:\n",
    "                Q4_win += 1\n",
    "            else:\n",
    "                Q4_loss += 1            \n",
    "        elif not team_is_home and opponent_index <= 60:\n",
    "            if team_won:\n",
    "                Q1_win += 1\n",
    "            else:\n",
    "                Q1_loss += 1\n",
    "        elif not team_is_home and opponent_index <= 120:\n",
    "            if team_won:\n",
    "                Q2_win += 1\n",
    "            else:\n",
    "                Q2_loss += 1\n",
    "        elif not team_is_home and opponent_index <= 240:\n",
    "            if team_won:\n",
    "                Q3_win += 1\n",
    "            else:\n",
    "                Q3_loss += 1\n",
    "        elif not team_is_home:\n",
    "            if team_won:\n",
    "                Q4_win += 1\n",
    "            else:\n",
    "                Q4_loss += 1\n",
    "            \n",
    "\n",
    "    # Store results for the team\n",
    "    quadrant_records[team] = {'Team': team, 'Q1': f\"{Q1_win}-{Q1_loss}\", 'Q2': f\"{Q2_win}-{Q2_loss}\", 'Q3': f\"{Q3_win}-{Q3_loss}\", 'Q4': f\"{Q4_win}-{Q4_loss}\"}\n",
    "quadrant_record_df = pd.DataFrame.from_dict(quadrant_records, orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kpi(completed_schedule, ending_data):\n",
    "    def get_team_rank(team):\n",
    "        match = ending_data.loc[ending_data[\"Team\"] == team]\n",
    "        return match.index[0] if not match.empty else len(ending_data) + 1\n",
    "\n",
    "    def get_opponent_rank(opponent):\n",
    "        match = ending_data.loc[ending_data[\"Team\"] == opponent]\n",
    "        return match.index[0] if not match.empty else len(ending_data) + 1\n",
    "\n",
    "    kpi_scores = []\n",
    "\n",
    "    for _, game in completed_schedule.iterrows():\n",
    "        team = game[\"Team\"]\n",
    "        opponent = game[\"Opponent\"]\n",
    "        home_team = game[\"home_team\"]\n",
    "        \n",
    "        # Team strength\n",
    "        team_rank = get_team_rank(team)\n",
    "        opponent_rank = get_opponent_rank(opponent)\n",
    "\n",
    "        # Opponent strength calculation\n",
    "        opponent_strength = 1 - (opponent_rank / (len(ending_data) + 1))\n",
    "        \n",
    "        # Determine if the team is home\n",
    "        is_home = team == home_team\n",
    "        \n",
    "        # Scoring margin\n",
    "        margin = game[\"home_score\"] - game[\"away_score\"]\n",
    "        if not is_home:\n",
    "            margin = -margin  # Flip if the team is away\n",
    "\n",
    "        # Win or loss multiplier\n",
    "        result_multiplier = 1.5 if margin > 0 else -1.5\n",
    "\n",
    "        # Margin factor\n",
    "        if margin > 0:\n",
    "            margin_factor = 1 + (min(margin, 20) / 20)\n",
    "        else:\n",
    "            margin_factor = max(0.1, 1 - (min(abs(margin), 20) / 20))\n",
    "\n",
    "        # Team strength adjustment\n",
    "        team_strength_adj = 1 - (team_rank / (len(ending_data) + 1))\n",
    "\n",
    "        # Adjusted KPI formula\n",
    "        adj_grv = (opponent_strength * result_multiplier * margin_factor / 1.5) * (1 + (team_strength_adj / 2))\n",
    "        \n",
    "        # Store result\n",
    "        kpi_scores.append({\"Team\": team, \"KPI_Score\": adj_grv})\n",
    "\n",
    "    # Convert to DataFrame and get average per team\n",
    "    kpi_df = pd.DataFrame(kpi_scores)\n",
    "    kpi_avg = kpi_df.groupby(\"Team\")[\"KPI_Score\"].mean().reset_index()\n",
    "\n",
    "    return kpi_avg\n",
    "\n",
    "# Call function\n",
    "kpi_results = calculate_kpi(completed_schedule, ending_data).sort_values('KPI_Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.merge(ending_data, team_expected_wins[['Team', 'expected_wins']], on='Team', how='left')\n",
    "df_2 = pd.merge(df_1, avg_team_expected_wins[['Team', 'avg_expected_wins']], on='Team', how='left')\n",
    "df_3 = pd.merge(df_2, rem_avg_expected_wins[['Team', 'rem_avg_expected_wins']], on='Team', how='left')\n",
    "df_4 = pd.merge(df_3, elo_data[['Team', 'ELO']], on='Team', how='left')\n",
    "df_5 = pd.merge(df_4, kpi_results, on='Team', how='left')\n",
    "stats_and_metrics = pd.merge(df_5, quadrant_record_df, on='Team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_and_metrics['wins_above_expected'] = round(stats_and_metrics['Wins'] - stats_and_metrics['avg_expected_wins'],2)\n",
    "stats_and_metrics['SOR'] = stats_and_metrics['wins_above_expected'].rank(method='min', ascending=False)\n",
    "max_SOR = stats_and_metrics['SOR'].max()\n",
    "stats_and_metrics['SOR'].fillna(max_SOR + 1, inplace=True)\n",
    "stats_and_metrics['SOR'] = stats_and_metrics['SOR'].astype(int)\n",
    "stats_and_metrics = stats_and_metrics.sort_values('SOR').reset_index(drop=True)\n",
    "\n",
    "stats_and_metrics['RemSOS'] = stats_and_metrics['rem_avg_expected_wins'].rank(method='min', ascending=True)\n",
    "max_remSOS = stats_and_metrics['RemSOS'].max()\n",
    "stats_and_metrics['RemSOS'].fillna(max_remSOS + 1, inplace=True)\n",
    "stats_and_metrics['RemSOS'] = stats_and_metrics['RemSOS'].astype(int)\n",
    "stats_and_metrics = stats_and_metrics.sort_values('RemSOS').reset_index(drop=True)\n",
    "\n",
    "stats_and_metrics['SOS'] = stats_and_metrics['avg_expected_wins'].rank(method='min', ascending=True)\n",
    "max_SOS = stats_and_metrics['SOS'].max()\n",
    "stats_and_metrics['SOS'].fillna(max_SOS + 1, inplace=True)\n",
    "stats_and_metrics['SOS'] = stats_and_metrics['SOS'].astype(int)\n",
    "stats_and_metrics = stats_and_metrics.sort_values('SOS').reset_index(drop=True)\n",
    "\n",
    "stats_and_metrics['ELO'].fillna(1200, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble_rating = stats_and_metrics.loc[(stats_and_metrics[\"SOR\"] >= 32) & (stats_and_metrics[\"SOR\"] <= 40), \"Rating\"].mean()\n",
    "bubble_expected_wins = completed_schedule.groupby('Team').apply(calculate_average_expected_wins, bubble_rating).reset_index(drop=True)\n",
    "bubble_expected_wins.rename(columns={\"avg_expected_wins\": \"bubble_expected_wins\"}, inplace=True)\n",
    "\n",
    "stats_and_metrics = pd.merge(stats_and_metrics, bubble_expected_wins, on='Team', how='left')\n",
    "\n",
    "stats_and_metrics['wins_above_bubble'] = round(stats_and_metrics['Wins'] - stats_and_metrics['bubble_expected_wins'],2)\n",
    "stats_and_metrics['WAB'] = stats_and_metrics['wins_above_bubble'].rank(method='min', ascending=False)\n",
    "max_WAB = stats_and_metrics['WAB'].max()\n",
    "stats_and_metrics['WAB'].fillna(max_WAB + 1, inplace=True)\n",
    "stats_and_metrics['WAB'] = stats_and_metrics['WAB'].astype(int)\n",
    "stats_and_metrics = stats_and_metrics.sort_values('WAB').reset_index(drop=True)\n",
    "\n",
    "stats_and_metrics['KPI'] = stats_and_metrics['KPI_Score'].rank(method='min', ascending=False)\n",
    "max_KPI = stats_and_metrics['KPI'].max()\n",
    "stats_and_metrics['KPI'].fillna(max_KPI + 1, inplace=True)\n",
    "stats_and_metrics['KPI'] = stats_and_metrics['KPI'].astype(int)\n",
    "\n",
    "stats_and_metrics['AVG'] = round(stats_and_metrics[['KPI', 'WAB', 'SOR']].mean(axis=1),1)\n",
    "\n",
    "stats_and_metrics.fillna(0, inplace=True)\n",
    "stats_and_metrics = stats_and_metrics.sort_values('Rating', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictionary\n",
    "\n",
    "- G: Games\n",
    "- AB: At Bats\n",
    "- H: Hits\n",
    "- BA: Batting Average\n",
    "- HPG: Hits Per Game\n",
    "- ABPG: At Bats Per Game\n",
    "- HPAB: Hits Per At Bat\n",
    "- BB: Walks\n",
    "- BBPG: Walks Per Game\n",
    "- DP: Double Plays\n",
    "- DPPG: Double Plays Per Game\n",
    "- IP: Innings Pitched\n",
    "- RA: Runs Allowed\n",
    "- ER: Earned Runs\n",
    "- ERA: Earned Runs Allowed\n",
    "- PO: Put Outs\n",
    "- A: Assists\n",
    "- E: Errors\n",
    "- PCT: Fielding Percentage\n",
    "- APG: Assists Per Game\n",
    "- EPG: Errors Per Game\n",
    "- HA: Hits Allowed\n",
    "- HAPG: Hits Allowed Per Game\n",
    "- HR: Home Runs Hit\n",
    "- HRPG: Home Runs Hit Per Game\n",
    "- HBP: Hit By Pitch\n",
    "- OBP: On Base Percentage\n",
    "- HBPPG: Hit By Pitch Per Game\n",
    "- RS: Runs Scored\n",
    "- RPG: Runs Scored Per Game\n",
    "- SB: Sacrifice Bunts\n",
    "- SBPG: Sacrifice Bunts Per Game\n",
    "- SF: Sacrifice Flies\n",
    "- SFPG: Sacrifice Flies Per Game\n",
    "- TB: Total Bases\n",
    "- SLG: Slugging Percentage\n",
    "- STL: Stolen Bases\n",
    "- CS: Caught Stealing\n",
    "- STLP: Stolen Bases Success Percentage\n",
    "- STLPG: Stolen Bases Per Game\n",
    "- CSPG: Caught Stealing Per Game\n",
    "- SAPG: Stealing Attempts Per Game\n",
    "- SO: Pitching Strike Outs\n",
    "- PBB: Pitching Walks\n",
    "- KBB: Strikeouts to Walk Ratio\n",
    "- KP9: Strikeouts Per Nine\n",
    "- WP9: Walks Allowed Per Nine\n",
    "- WHIP: Walks Hits Over Innings Pitched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape all stats at once\n",
    "# for stat_name, url in stat_links.items():\n",
    "#     print(f\"Scraping: {stat_name} ({url})\")\n",
    "    \n",
    "#     # Get stats page content\n",
    "#     soup = get_soup(url)\n",
    "    \n",
    "#     # Locate table\n",
    "#     table = soup.find(\"table\")\n",
    "#     if not table:\n",
    "#         print(f\"No table found for {stat_name}\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract table headers\n",
    "#     headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "\n",
    "#     # Extract table rows\n",
    "#     data = []\n",
    "#     for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "#         cols = row.find_all(\"td\")\n",
    "#         data.append([col.text.strip() for col in cols])\n",
    "\n",
    "#     # Convert to DataFrame and save\n",
    "#     df = pd.DataFrame(data, columns=headers)\n",
    "#     # df.to_csv(f\"{stat_name}.csv\", index=False)\n",
    "#     print(f\"Saved {stat_name}.csv\")\n",
    "\n",
    "# print(\"Scraping completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power_ratings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
